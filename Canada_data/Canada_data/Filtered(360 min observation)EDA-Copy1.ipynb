{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "aee84fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85ed7a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this csv contains all the data from canadian weather stations from 1917-2017\n",
    "df = pd.read_csv('./data/canada_all_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "502c7a79",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Unnamed: 0'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-9817f74d6b7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Unnamed: 0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'decade'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Year'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m10\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4161\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[1;36m1.0\u001b[0m     \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4162\u001b[0m         \"\"\"\n\u001b[1;32m-> 4163\u001b[1;33m         return super().drop(\n\u001b[0m\u001b[0;32m   4164\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4165\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3885\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3886\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3887\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3888\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3889\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3919\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3920\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3921\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3922\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   5280\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5281\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5282\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5283\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5284\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Unnamed: 0'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# the csv was saved with an index droping the index column\n",
    "df.drop(columns='Unnamed: 0', inplace = True)\n",
    "df['decade'] = df['Year']//10 *10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e46f8ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/29836836/how-do-i-filter-a-pandas-dataframe-based-on-value-counts\n",
    "\n",
    "# Makes the dataframe consist of only stations with more than 30 years worth of observations\n",
    "df = df.groupby(\"Clim_ID\").filter(lambda x: len(x) > 360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9118f2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets the features for the model to be all numerical values in our dataset, and sets out target to what decade the data came from\n",
    "\n",
    "X= df.drop(columns= ['Stn_Name', 'Prov', 'Clim_ID', 'decade', 'Year'])\n",
    "y = df['decade']\n",
    "\n",
    "X_train, X_test,y_train,y_test = train_test_split(X,y, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b797804",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "\n",
    "X_train_sc = sc.fit_transform(X_train)\n",
    "X_test_sc = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cad1016d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2388341578227841, 0.22741238372639294)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf =RandomForestClassifier(max_depth=10)\n",
    "rf.fit(X_train_sc,y_train)\n",
    "rf.score(X_train_sc,y_train), rf.score(X_test_sc,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00fdd934",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23806369474628744 0.22611575683547872 5\n",
      "0.23989902548948677 0.22807009301888564 6\n",
      "0.2385037356497337 0.2271070186977356 7\n",
      "0.2405504739444108 0.2277459362961571 8\n",
      "0.23903773546494783 0.22781170722540636 9\n",
      "0.23990528941693798 0.2282861975007047 10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-1762cfee7b18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdepth\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mrf\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_sc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_sc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_sc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[1;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[1;31m# since correctness does not rely on using threads.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[0;32m    387\u001b[0m                              \u001b[1;33m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'threads'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m                 delayed(_parallel_build_trees)(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1044\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1045\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    166\u001b[0m                                                         indices=indices)\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    888\u001b[0m         \"\"\"\n\u001b[0;32m    889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 890\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m    891\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    373\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for depth in range(5,20):\n",
    "    rf =RandomForestClassifier(max_depth=10)\n",
    "    rf.fit(X_train_sc,y_train)\n",
    "    print(rf.score(X_train_sc,y_train), rf.score(X_test_sc,y_test), depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a116449e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999859061632348 0.4155642206144884 11\n"
     ]
    }
   ],
   "source": [
    "    rf =RandomForestClassifier()\n",
    "    rf.fit(X_train_sc,y_train)\n",
    "    print(rf.score(X_train_sc,y_train), rf.score(X_test_sc,y_test), depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f81a0487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9999874721450976, 0.41590247110777034)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf =RandomForestClassifier(max_depth=100)\n",
    "rf.fit(X_train_sc,y_train)\n",
    "rf.score(X_train_sc,y_train), rf.score(X_test_sc,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66a49131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 44s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.99712329131804, 0.4117072254063704)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "rf =RandomForestClassifier(max_depth=30)\n",
    "rf.fit(X_train_sc,y_train)\n",
    "rf.score(X_train_sc,y_train), rf.score(X_test_sc,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78527d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 47s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.681072133822546, 0.35350465094428263)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "rf =RandomForestClassifier(max_depth=20)\n",
    "rf.fit(X_train_sc,y_train)\n",
    "rf.score(X_train_sc,y_train), rf.score(X_test_sc,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4e64568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7470218939924238 0.36532462651508035 21\n",
      "0.8571636623304629 0.3825894954430142 23\n",
      "0.9335851432168712 0.3953020764821949 25\n",
      "0.9709839220642147 0.40241003476463405 27\n",
      "0.9923705363644478 0.408799210748849 29\n"
     ]
    }
   ],
   "source": [
    "for depth in range(21,30,2):\n",
    "    rf =RandomForestClassifier(max_depth=depth)\n",
    "    rf.fit(X_train_sc,y_train)\n",
    "    print(rf.score(X_train_sc,y_train), rf.score(X_test_sc,y_test), depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cac3a045",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dec ={'1910':0, '1920':1, '1930':2, '1940':3, '1950':4, '1960':5, '1970':6, '1980':7,\n",
    "       '1990':8, '2000':9, '2010':10}\n",
    "df['decade'] = df['Year']//10 *10\n",
    "df['decade'] = df['decade'].astype(str)\n",
    "df['decade']=   df['decade'].map(dec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6215ccc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X= df.drop(columns= ['Stn_Name', 'Prov', 'Clim_ID', 'decade', 'Year'])\n",
    "y = df['decade']\n",
    "\n",
    "y = to_categorical(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0348e369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(851437, 11)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0ea0086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test,y_train,y_test = train_test_split(X,y, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "98dd19b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "\n",
    "X_train_sc = sc.fit_transform(X_train)\n",
    "X_test_sc = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "87d86515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create our model\n",
    "nn= Sequential()\n",
    "# 1st hidden layer\n",
    "nn.add(Dense(32, input_dim=X_train_sc.shape[1], activation='relu'))\n",
    "nn.add(Dense(32, activation='relu'))\n",
    "# output layer\n",
    "nn.add(Dense(11, activation='softmax'))\n",
    "\n",
    "nn.compile(loss ='categorical_crossentropy', optimizer= 'adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b4aeb10a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "19956/19956 [==============================] - 10s 488us/step - loss: 2.1940 - acc: 0.1813 - val_loss: 2.1487 - val_acc: 0.1986\n",
      "Epoch 2/10\n",
      "19956/19956 [==============================] - 10s 485us/step - loss: 2.1446 - acc: 0.2018 - val_loss: 2.1402 - val_acc: 0.1995\n",
      "Epoch 3/10\n",
      "19956/19956 [==============================] - 10s 494us/step - loss: 2.1330 - acc: 0.2063 - val_loss: 2.1291 - val_acc: 0.2075\n",
      "Epoch 4/10\n",
      "19956/19956 [==============================] - 10s 496us/step - loss: 2.1261 - acc: 0.2098 - val_loss: 2.1246 - val_acc: 0.2091\n",
      "Epoch 5/10\n",
      "19956/19956 [==============================] - 10s 494us/step - loss: 2.1204 - acc: 0.2126 - val_loss: 2.1223 - val_acc: 0.2116\n",
      "Epoch 6/10\n",
      "19956/19956 [==============================] - 10s 493us/step - loss: 2.1166 - acc: 0.2139 - val_loss: 2.1218 - val_acc: 0.2096\n",
      "Epoch 7/10\n",
      "19956/19956 [==============================] - 10s 492us/step - loss: 2.1129 - acc: 0.2148 - val_loss: 2.1188 - val_acc: 0.2116\n",
      "Epoch 8/10\n",
      "19956/19956 [==============================] - 10s 490us/step - loss: 2.1111 - acc: 0.2153 - val_loss: 2.1120 - val_acc: 0.2146\n",
      "Epoch 9/10\n",
      "19956/19956 [==============================] - 10s 485us/step - loss: 2.1095 - acc: 0.2157 - val_loss: 2.1140 - val_acc: 0.2144\n",
      "Epoch 10/10\n",
      "19956/19956 [==============================] - 10s 484us/step - loss: 2.1100 - acc: 0.2161 - val_loss: 2.1081 - val_acc: 0.2166\n"
     ]
    }
   ],
   "source": [
    "results = nn.fit(X_train_sc, y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=10,\n",
    "                    validation_data=(X_test_sc,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "acd50b3b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "4989/4989 [==============================] - 6s 1ms/step - loss: 2.1696 - acc: 0.1924 - val_loss: 2.1005 - val_acc: 0.2238\n",
      "Epoch 2/16\n",
      "4989/4989 [==============================] - 6s 1ms/step - loss: 2.0836 - acc: 0.2315 - val_loss: 2.0447 - val_acc: 0.2494\n",
      "Epoch 3/16\n",
      "4989/4989 [==============================] - 6s 1ms/step - loss: 2.0268 - acc: 0.2572 - val_loss: 2.0043 - val_acc: 0.2685\n",
      "Epoch 4/16\n",
      "4989/4989 [==============================] - 6s 1ms/step - loss: 1.9835 - acc: 0.2769 - val_loss: 1.9696 - val_acc: 0.2829\n",
      "Epoch 5/16\n",
      "4989/4989 [==============================] - 6s 1ms/step - loss: 1.9443 - acc: 0.2934 - val_loss: 1.9496 - val_acc: 0.2916\n",
      "Epoch 6/16\n",
      "4989/4989 [==============================] - 6s 1ms/step - loss: 1.9156 - acc: 0.3062 - val_loss: 1.9283 - val_acc: 0.3006\n",
      "Epoch 7/16\n",
      "4989/4989 [==============================] - 6s 1ms/step - loss: 1.8924 - acc: 0.3147 - val_loss: 1.9229 - val_acc: 0.3045\n",
      "Epoch 8/16\n",
      "4989/4989 [==============================] - 6s 1ms/step - loss: 1.8718 - acc: 0.3235 - val_loss: 1.9030 - val_acc: 0.3131\n",
      "Epoch 9/16\n",
      "4989/4989 [==============================] - 6s 1ms/step - loss: 1.8555 - acc: 0.3305 - val_loss: 1.8964 - val_acc: 0.3157\n",
      "Epoch 10/16\n",
      "4989/4989 [==============================] - 6s 1ms/step - loss: 1.8390 - acc: 0.3370 - val_loss: 1.8848 - val_acc: 0.3197\n",
      "Epoch 11/16\n",
      "4989/4989 [==============================] - 6s 1ms/step - loss: 1.8265 - acc: 0.3422 - val_loss: 1.8836 - val_acc: 0.3226\n",
      "Epoch 12/16\n",
      "4989/4989 [==============================] - 6s 1ms/step - loss: 1.8160 - acc: 0.3469 - val_loss: 1.8743 - val_acc: 0.3250\n",
      "Epoch 13/16\n",
      "4989/4989 [==============================] - 6s 1ms/step - loss: 1.8027 - acc: 0.3513 - val_loss: 1.8694 - val_acc: 0.3273\n",
      "Epoch 14/16\n",
      "4989/4989 [==============================] - 6s 1ms/step - loss: 1.7954 - acc: 0.3541 - val_loss: 1.8610 - val_acc: 0.3311\n",
      "Epoch 15/16\n",
      "4989/4989 [==============================] - 6s 1ms/step - loss: 1.7865 - acc: 0.3580 - val_loss: 1.8571 - val_acc: 0.3337\n",
      "Epoch 16/16\n",
      "4989/4989 [==============================] - 6s 1ms/step - loss: 1.7765 - acc: 0.3611 - val_loss: 1.8569 - val_acc: 0.3351\n"
     ]
    }
   ],
   "source": [
    "# let's create our model\n",
    "nn= Sequential()\n",
    "# 1st hidden layer\n",
    "nn.add(Dense(256, input_dim=X_train_sc.shape[1], activation='relu'))\n",
    "nn.add(Dense(256, activation='relu'))\n",
    "nn.add(Dense(256, activation='relu'))\n",
    "\n",
    "# output layer\n",
    "nn.add(Dense(11, activation='softmax'))\n",
    "\n",
    "nn.compile(loss ='categorical_crossentropy', optimizer= 'adam', metrics=['acc'])\n",
    "\n",
    "results = nn.fit(X_train_sc, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=16,\n",
    "                    validation_data=(X_test_sc,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "deab7ee4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2495/2495 [==============================] - 16s 6ms/step - loss: 2.1677 - acc: 0.1934 - val_loss: 2.0992 - val_acc: 0.2239\n",
      "Epoch 2/16\n",
      "2495/2495 [==============================] - 14s 6ms/step - loss: 2.0739 - acc: 0.2360 - val_loss: 2.0259 - val_acc: 0.2594\n",
      "Epoch 3/16\n",
      "2495/2495 [==============================] - 15s 6ms/step - loss: 2.0070 - acc: 0.2666 - val_loss: 1.9757 - val_acc: 0.2813\n",
      "Epoch 4/16\n",
      "2495/2495 [==============================] - 14s 6ms/step - loss: 1.9488 - acc: 0.2922 - val_loss: 1.9406 - val_acc: 0.2964\n",
      "Epoch 5/16\n",
      "2495/2495 [==============================] - 14s 6ms/step - loss: 1.8975 - acc: 0.3139 - val_loss: 1.9050 - val_acc: 0.3123\n",
      "Epoch 6/16\n",
      "2495/2495 [==============================] - 14s 6ms/step - loss: 1.8583 - acc: 0.3277 - val_loss: 1.8764 - val_acc: 0.3245\n",
      "Epoch 7/16\n",
      "2495/2495 [==============================] - 15s 6ms/step - loss: 1.8244 - acc: 0.3425 - val_loss: 1.8574 - val_acc: 0.3316\n",
      "Epoch 8/16\n",
      "2495/2495 [==============================] - 15s 6ms/step - loss: 1.7937 - acc: 0.3554 - val_loss: 1.8471 - val_acc: 0.3364\n",
      "Epoch 9/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.7687 - acc: 0.3637 - val_loss: 1.8361 - val_acc: 0.3420\n",
      "Epoch 10/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.7497 - acc: 0.3717 - val_loss: 1.8278 - val_acc: 0.3447\n",
      "Epoch 11/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.7280 - acc: 0.3791 - val_loss: 1.8194 - val_acc: 0.3491\n",
      "Epoch 12/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.7097 - acc: 0.3883 - val_loss: 1.8077 - val_acc: 0.3548\n",
      "Epoch 13/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.6923 - acc: 0.3928 - val_loss: 1.8089 - val_acc: 0.3548\n",
      "Epoch 14/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.6766 - acc: 0.3993 - val_loss: 1.8027 - val_acc: 0.3587\n",
      "Epoch 15/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.6632 - acc: 0.4046 - val_loss: 1.8014 - val_acc: 0.3596\n",
      "Epoch 16/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.6499 - acc: 0.4088 - val_loss: 1.7958 - val_acc: 0.3625\n"
     ]
    }
   ],
   "source": [
    "# let's create our model\n",
    "nn= Sequential()\n",
    "# 1st hidden layer\n",
    "nn.add(Dense(512, input_dim=X_train_sc.shape[1], activation='relu'))\n",
    "nn.add(Dense(512, activation='relu'))\n",
    "nn.add(Dense(512, activation='relu'))\n",
    "\n",
    "# output layer\n",
    "nn.add(Dense(11, activation='softmax'))\n",
    "\n",
    "nn.compile(loss ='categorical_crossentropy', optimizer= 'adam', metrics=['acc'])\n",
    "\n",
    "results = nn.fit(X_train_sc, y_train,\n",
    "                    batch_size=256,\n",
    "                    epochs=16,\n",
    "                    validation_data=(X_test_sc,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cc37ca48",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2495/2495 [==============================] - 20s 8ms/step - loss: 2.1690 - acc: 0.1918 - val_loss: 2.0881 - val_acc: 0.2283\n",
      "Epoch 2/16\n",
      "2495/2495 [==============================] - 20s 8ms/step - loss: 2.0690 - acc: 0.2366 - val_loss: 2.0218 - val_acc: 0.2578\n",
      "Epoch 3/16\n",
      "2495/2495 [==============================] - 18s 7ms/step - loss: 1.9864 - acc: 0.2749 - val_loss: 1.9509 - val_acc: 0.2909\n",
      "Epoch 4/16\n",
      "2495/2495 [==============================] - 18s 7ms/step - loss: 1.9109 - acc: 0.3063 - val_loss: 1.9001 - val_acc: 0.3123\n",
      "Epoch 5/16\n",
      "2495/2495 [==============================] - 18s 7ms/step - loss: 1.8513 - acc: 0.3306 - val_loss: 1.8656 - val_acc: 0.3278\n",
      "Epoch 6/16\n",
      "2495/2495 [==============================] - 19s 8ms/step - loss: 1.8015 - acc: 0.3509 - val_loss: 1.8297 - val_acc: 0.3402\n",
      "Epoch 7/16\n",
      "2495/2495 [==============================] - 20s 8ms/step - loss: 1.7594 - acc: 0.3657 - val_loss: 1.8211 - val_acc: 0.3470\n",
      "Epoch 8/16\n",
      "2495/2495 [==============================] - 20s 8ms/step - loss: 1.7261 - acc: 0.3782 - val_loss: 1.8054 - val_acc: 0.3522\n",
      "Epoch 9/16\n",
      "2495/2495 [==============================] - 20s 8ms/step - loss: 1.6932 - acc: 0.3905 - val_loss: 1.7892 - val_acc: 0.3614\n",
      "Epoch 10/16\n",
      "2495/2495 [==============================] - 19s 8ms/step - loss: 1.6656 - acc: 0.4021 - val_loss: 1.7932 - val_acc: 0.3599\n",
      "Epoch 11/16\n",
      "2495/2495 [==============================] - 19s 8ms/step - loss: 1.6390 - acc: 0.4106 - val_loss: 1.7787 - val_acc: 0.3667\n",
      "Epoch 12/16\n",
      "2495/2495 [==============================] - 19s 8ms/step - loss: 1.6167 - acc: 0.4204 - val_loss: 1.7707 - val_acc: 0.3712\n",
      "Epoch 13/16\n",
      "2495/2495 [==============================] - 20s 8ms/step - loss: 1.5928 - acc: 0.4280 - val_loss: 1.7822 - val_acc: 0.3690\n",
      "Epoch 14/16\n",
      "2495/2495 [==============================] - 19s 8ms/step - loss: 1.5717 - acc: 0.4361 - val_loss: 1.7733 - val_acc: 0.3749\n",
      "Epoch 15/16\n",
      "2495/2495 [==============================] - 18s 7ms/step - loss: 1.5510 - acc: 0.4436 - val_loss: 1.7758 - val_acc: 0.3743\n",
      "Epoch 16/16\n",
      "2495/2495 [==============================] - 18s 7ms/step - loss: 1.5348 - acc: 0.4479 - val_loss: 1.7784 - val_acc: 0.3769\n"
     ]
    }
   ],
   "source": [
    "# let's create our model\n",
    "nn= Sequential()\n",
    "# 1st hidden layer\n",
    "nn.add(Dense(512, input_dim=X_train_sc.shape[1], activation='relu'))\n",
    "nn.add(Dense(512, activation='relu'))\n",
    "nn.add(Dense(512, activation='relu'))\n",
    "nn.add(Dense(512, activation='relu'))\n",
    "\n",
    "# output layer\n",
    "nn.add(Dense(11, activation='softmax'))\n",
    "\n",
    "nn.compile(loss ='categorical_crossentropy', optimizer= 'adam', metrics=['acc'])\n",
    "\n",
    "results = nn.fit(X_train_sc, y_train,\n",
    "                    batch_size=256,\n",
    "                    epochs=16,\n",
    "                    validation_data=(X_test_sc,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e1b46dfc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2495/2495 [==============================] - 40s 16ms/step - loss: 2.1671 - acc: 0.1921 - val_loss: 2.0874 - val_acc: 0.2310\n",
      "Epoch 2/16\n",
      "2495/2495 [==============================] - 38s 15ms/step - loss: 2.0628 - acc: 0.2402 - val_loss: 2.0088 - val_acc: 0.2655\n",
      "Epoch 3/16\n",
      "2495/2495 [==============================] - 39s 16ms/step - loss: 1.9782 - acc: 0.2789 - val_loss: 1.9392 - val_acc: 0.2959\n",
      "Epoch 4/16\n",
      "2495/2495 [==============================] - 40s 16ms/step - loss: 1.9065 - acc: 0.3096 - val_loss: 1.8964 - val_acc: 0.3156\n",
      "Epoch 5/16\n",
      "2495/2495 [==============================] - 39s 16ms/step - loss: 1.8472 - acc: 0.3322 - val_loss: 1.8740 - val_acc: 0.3255\n",
      "Epoch 6/16\n",
      "2495/2495 [==============================] - 40s 16ms/step - loss: 1.7978 - acc: 0.3516 - val_loss: 1.8393 - val_acc: 0.3391\n",
      "Epoch 7/16\n",
      "2495/2495 [==============================] - 38s 15ms/step - loss: 1.7567 - acc: 0.3672 - val_loss: 1.8185 - val_acc: 0.3489\n",
      "Epoch 8/16\n",
      "2495/2495 [==============================] - 41s 16ms/step - loss: 1.7204 - acc: 0.3810 - val_loss: 1.8090 - val_acc: 0.3539\n",
      "Epoch 9/16\n",
      "2495/2495 [==============================] - 40s 16ms/step - loss: 1.6916 - acc: 0.3930 - val_loss: 1.7959 - val_acc: 0.3586\n",
      "Epoch 10/16\n",
      "2495/2495 [==============================] - 40s 16ms/step - loss: 1.6638 - acc: 0.4033 - val_loss: 1.7936 - val_acc: 0.3632\n",
      "Epoch 11/16\n",
      "2495/2495 [==============================] - 41s 16ms/step - loss: 1.6366 - acc: 0.4124 - val_loss: 1.7859 - val_acc: 0.3675\n",
      "Epoch 12/16\n",
      "2495/2495 [==============================] - 41s 16ms/step - loss: 1.6116 - acc: 0.4222 - val_loss: 1.7864 - val_acc: 0.3699\n",
      "Epoch 13/16\n",
      "2495/2495 [==============================] - 40s 16ms/step - loss: 1.5892 - acc: 0.4309 - val_loss: 1.7789 - val_acc: 0.3733\n",
      "Epoch 14/16\n",
      "2495/2495 [==============================] - 38s 15ms/step - loss: 1.5682 - acc: 0.4378 - val_loss: 1.7773 - val_acc: 0.3725\n",
      "Epoch 15/16\n",
      "2495/2495 [==============================] - 37s 15ms/step - loss: 1.5473 - acc: 0.4453 - val_loss: 1.7798 - val_acc: 0.3771\n",
      "Epoch 16/16\n",
      "2495/2495 [==============================] - 39s 15ms/step - loss: 1.5305 - acc: 0.4519 - val_loss: 1.7801 - val_acc: 0.3786\n"
     ]
    }
   ],
   "source": [
    "# let's create our model\n",
    "nn= Sequential()\n",
    "# 1st hidden layer\n",
    "nn.add(Dense(1024, input_dim=X_train_sc.shape[1], activation='relu'))\n",
    "nn.add(Dense(1024, activation='relu'))\n",
    "nn.add(Dense(1024, activation='relu'))\n",
    "\n",
    "\n",
    "# output layer\n",
    "nn.add(Dense(11, activation='softmax'))\n",
    "\n",
    "nn.compile(loss ='categorical_crossentropy', optimizer= 'adam', metrics=['acc'])\n",
    "\n",
    "results = nn.fit(X_train_sc, y_train,\n",
    "                    batch_size=256,\n",
    "                    epochs=16,\n",
    "                    validation_data=(X_test_sc,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "68b16405",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.1681 - acc: 0.1933 - val_loss: 2.0906 - val_acc: 0.2277\n",
      "Epoch 2/32\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.0720 - acc: 0.2348 - val_loss: 2.0308 - val_acc: 0.2549\n",
      "Epoch 3/32\n",
      "2495/2495 [==============================] - 14s 5ms/step - loss: 2.0049 - acc: 0.2673 - val_loss: 1.9753 - val_acc: 0.2797\n",
      "Epoch 4/32\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.9437 - acc: 0.2933 - val_loss: 1.9365 - val_acc: 0.2985\n",
      "Epoch 5/32\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.8974 - acc: 0.3122 - val_loss: 1.8983 - val_acc: 0.3138\n",
      "Epoch 6/32\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.8553 - acc: 0.3293 - val_loss: 1.8771 - val_acc: 0.3236\n",
      "Epoch 7/32\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.8224 - acc: 0.3437 - val_loss: 1.8665 - val_acc: 0.3282\n",
      "Epoch 8/32\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.7963 - acc: 0.3532 - val_loss: 1.8463 - val_acc: 0.3360\n",
      "Epoch 9/32\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.7700 - acc: 0.3633 - val_loss: 1.8384 - val_acc: 0.3407\n",
      "Epoch 10/32\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.7466 - acc: 0.3732 - val_loss: 1.8325 - val_acc: 0.3429\n",
      "Epoch 11/32\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.7283 - acc: 0.3788 - val_loss: 1.8166 - val_acc: 0.3496\n",
      "Epoch 12/32\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.7111 - acc: 0.3863 - val_loss: 1.8119 - val_acc: 0.3539\n",
      "Epoch 13/32\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.6899 - acc: 0.3937 - val_loss: 1.8165 - val_acc: 0.3521\n",
      "Epoch 14/32\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.6780 - acc: 0.3987 - val_loss: 1.8090 - val_acc: 0.3560\n",
      "Epoch 15/32\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.6632 - acc: 0.4034 - val_loss: 1.8024 - val_acc: 0.3594\n",
      "Epoch 16/32\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.6504 - acc: 0.4091 - val_loss: 1.7939 - val_acc: 0.3618\n",
      "Epoch 17/32\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.6386 - acc: 0.4136 - val_loss: 1.7950 - val_acc: 0.3628\n",
      "Epoch 18/32\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.6245 - acc: 0.4185 - val_loss: 1.7968 - val_acc: 0.3632\n",
      "Epoch 19/32\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.6124 - acc: 0.4233 - val_loss: 1.7882 - val_acc: 0.3668\n",
      "Epoch 20/32\n",
      "2495/2495 [==============================] - 14s 6ms/step - loss: 1.6038 - acc: 0.4254 - val_loss: 1.7917 - val_acc: 0.3670\n",
      "Epoch 21/32\n",
      "2495/2495 [==============================] - 14s 6ms/step - loss: 1.5928 - acc: 0.4306 - val_loss: 1.7933 - val_acc: 0.3669\n",
      "Epoch 22/32\n",
      "2495/2495 [==============================] - 14s 6ms/step - loss: 1.5846 - acc: 0.4338 - val_loss: 1.8085 - val_acc: 0.3649\n",
      "Epoch 23/32\n",
      "2495/2495 [==============================] - 14s 6ms/step - loss: 1.5762 - acc: 0.4356 - val_loss: 1.7973 - val_acc: 0.3704\n",
      "Epoch 24/32\n",
      "2495/2495 [==============================] - 14s 6ms/step - loss: 1.5657 - acc: 0.4398 - val_loss: 1.7975 - val_acc: 0.3695\n",
      "Epoch 25/32\n",
      "2495/2495 [==============================] - 14s 6ms/step - loss: 1.5586 - acc: 0.4422 - val_loss: 1.8006 - val_acc: 0.3701\n",
      "Epoch 26/32\n",
      "2495/2495 [==============================] - 14s 6ms/step - loss: 1.5475 - acc: 0.4462 - val_loss: 1.8002 - val_acc: 0.3714\n",
      "Epoch 27/32\n",
      "2495/2495 [==============================] - 14s 6ms/step - loss: 1.5403 - acc: 0.4492 - val_loss: 1.8011 - val_acc: 0.3709\n",
      "Epoch 28/32\n",
      "2495/2495 [==============================] - 14s 6ms/step - loss: 1.5346 - acc: 0.4512 - val_loss: 1.7977 - val_acc: 0.3735\n",
      "Epoch 29/32\n",
      "2495/2495 [==============================] - 14s 6ms/step - loss: 1.5232 - acc: 0.4554 - val_loss: 1.8068 - val_acc: 0.3743\n",
      "Epoch 30/32\n",
      "2495/2495 [==============================] - 15s 6ms/step - loss: 1.5187 - acc: 0.4577 - val_loss: 1.8059 - val_acc: 0.3737\n",
      "Epoch 31/32\n",
      "2495/2495 [==============================] - 14s 6ms/step - loss: 1.5134 - acc: 0.4579 - val_loss: 1.8036 - val_acc: 0.3755\n",
      "Epoch 32/32\n",
      "2495/2495 [==============================] - 14s 6ms/step - loss: 1.5056 - acc: 0.4621 - val_loss: 1.8091 - val_acc: 0.3740\n"
     ]
    }
   ],
   "source": [
    "# let's create our model\n",
    "nn= Sequential()\n",
    "# 1st hidden layer\n",
    "nn.add(Dense(512, input_dim=X_train_sc.shape[1], activation='relu'))\n",
    "nn.add(Dense(512, activation='relu'))\n",
    "nn.add(Dense(512, activation='relu'))\n",
    "\n",
    "# output layer\n",
    "nn.add(Dense(11, activation='softmax'))\n",
    "\n",
    "nn.compile(loss ='categorical_crossentropy', optimizer= 'adam', metrics=['acc'])\n",
    "\n",
    "results = nn.fit(X_train_sc, y_train,\n",
    "                    batch_size=256,\n",
    "                    epochs=32,\n",
    "                    validation_data=(X_test_sc,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f636d817",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "2495/2495 [==============================] - 75s 30ms/step - loss: 2.1687 - acc: 0.1937 - val_loss: 2.0949 - val_acc: 0.2239\n",
      "Epoch 2/32\n",
      "2495/2495 [==============================] - 76s 31ms/step - loss: 2.0801 - acc: 0.2314 - val_loss: 2.0427 - val_acc: 0.2464\n",
      "Epoch 3/32\n",
      "2495/2495 [==============================] - 77s 31ms/step - loss: 2.0266 - acc: 0.2563 - val_loss: 2.0020 - val_acc: 0.2673\n",
      "Epoch 4/32\n",
      "2495/2495 [==============================] - 76s 30ms/step - loss: 1.9774 - acc: 0.2791 - val_loss: 1.9647 - val_acc: 0.2854\n",
      "Epoch 5/32\n",
      "2495/2495 [==============================] - 76s 30ms/step - loss: 1.9358 - acc: 0.2965 - val_loss: 1.9385 - val_acc: 0.2969\n",
      "Epoch 6/32\n",
      "2495/2495 [==============================] - 76s 31ms/step - loss: 1.9039 - acc: 0.3103 - val_loss: 1.9197 - val_acc: 0.3056\n",
      "Epoch 7/32\n",
      "2495/2495 [==============================] - 77s 31ms/step - loss: 1.8759 - acc: 0.3204 - val_loss: 1.9040 - val_acc: 0.3120\n",
      "Epoch 8/32\n",
      "2495/2495 [==============================] - 78s 31ms/step - loss: 1.8492 - acc: 0.3309 - val_loss: 1.8878 - val_acc: 0.3188\n",
      "Epoch 9/32\n",
      "2495/2495 [==============================] - 77s 31ms/step - loss: 1.8256 - acc: 0.3392 - val_loss: 1.8745 - val_acc: 0.3256\n",
      "Epoch 10/32\n",
      "2495/2495 [==============================] - 77s 31ms/step - loss: 1.8058 - acc: 0.3490 - val_loss: 1.8632 - val_acc: 0.3293\n",
      "Epoch 11/32\n",
      "2495/2495 [==============================] - 77s 31ms/step - loss: 1.7873 - acc: 0.3564 - val_loss: 1.8566 - val_acc: 0.3344\n",
      "Epoch 12/32\n",
      "2495/2495 [==============================] - 77s 31ms/step - loss: 1.7686 - acc: 0.3637 - val_loss: 1.8442 - val_acc: 0.3383\n",
      "Epoch 13/32\n",
      "2495/2495 [==============================] - 76s 31ms/step - loss: 1.7520 - acc: 0.3703 - val_loss: 1.8362 - val_acc: 0.3427\n",
      "Epoch 14/32\n",
      "2495/2495 [==============================] - 77s 31ms/step - loss: 1.7365 - acc: 0.3759 - val_loss: 1.8313 - val_acc: 0.3443\n",
      "Epoch 15/32\n",
      "2495/2495 [==============================] - 76s 31ms/step - loss: 1.7196 - acc: 0.3819 - val_loss: 1.8286 - val_acc: 0.3468\n",
      "Epoch 16/32\n",
      "2495/2495 [==============================] - 77s 31ms/step - loss: 1.7029 - acc: 0.3884 - val_loss: 1.8216 - val_acc: 0.3498\n",
      "Epoch 17/32\n",
      "2495/2495 [==============================] - 77s 31ms/step - loss: 1.6935 - acc: 0.3912 - val_loss: 1.8201 - val_acc: 0.3513\n",
      "Epoch 18/32\n",
      "2495/2495 [==============================] - 77s 31ms/step - loss: 1.6806 - acc: 0.3965 - val_loss: 1.8124 - val_acc: 0.3563\n",
      "Epoch 19/32\n",
      "2495/2495 [==============================] - 77s 31ms/step - loss: 1.6689 - acc: 0.4007 - val_loss: 1.8220 - val_acc: 0.3526\n",
      "Epoch 20/32\n",
      "2495/2495 [==============================] - 76s 31ms/step - loss: 1.6571 - acc: 0.4055 - val_loss: 1.8125 - val_acc: 0.3566\n",
      "Epoch 21/32\n",
      "2495/2495 [==============================] - 76s 31ms/step - loss: 1.6433 - acc: 0.4101 - val_loss: 1.8148 - val_acc: 0.3566\n",
      "Epoch 22/32\n",
      "2495/2495 [==============================] - 76s 30ms/step - loss: 1.6317 - acc: 0.4142 - val_loss: 1.8126 - val_acc: 0.3590\n",
      "Epoch 23/32\n",
      "2495/2495 [==============================] - 80s 32ms/step - loss: 1.6208 - acc: 0.4187 - val_loss: 1.8103 - val_acc: 0.3602\n",
      "Epoch 24/32\n",
      "2495/2495 [==============================] - 80s 32ms/step - loss: 1.6080 - acc: 0.4238 - val_loss: 1.8112 - val_acc: 0.3635\n",
      "Epoch 25/32\n",
      "2495/2495 [==============================] - 77s 31ms/step - loss: 1.5956 - acc: 0.4286 - val_loss: 1.8154 - val_acc: 0.3606\n",
      "Epoch 26/32\n",
      "2495/2495 [==============================] - 77s 31ms/step - loss: 1.5882 - acc: 0.4305 - val_loss: 1.8113 - val_acc: 0.3665\n",
      "Epoch 27/32\n",
      "2495/2495 [==============================] - 77s 31ms/step - loss: 1.5790 - acc: 0.4341 - val_loss: 1.8153 - val_acc: 0.3639\n",
      "Epoch 28/32\n",
      "2495/2495 [==============================] - 77s 31ms/step - loss: 1.5693 - acc: 0.4368 - val_loss: 1.8220 - val_acc: 0.3628\n",
      "Epoch 29/32\n",
      "2495/2495 [==============================] - 77s 31ms/step - loss: 1.5618 - acc: 0.4406 - val_loss: 1.8096 - val_acc: 0.3682\n",
      "Epoch 30/32\n",
      "2495/2495 [==============================] - 77s 31ms/step - loss: 1.5487 - acc: 0.4449 - val_loss: 1.8192 - val_acc: 0.3676\n",
      "Epoch 31/32\n",
      "2495/2495 [==============================] - 77s 31ms/step - loss: 1.5440 - acc: 0.4473 - val_loss: 1.8184 - val_acc: 0.3676\n",
      "Epoch 32/32\n",
      "2495/2495 [==============================] - 77s 31ms/step - loss: 1.5310 - acc: 0.4521 - val_loss: 1.8246 - val_acc: 0.3676\n"
     ]
    }
   ],
   "source": [
    "# let's create our model\n",
    "nn= Sequential()\n",
    "# 1st hidden layer\n",
    "nn.add(Dense(2048, input_dim=X_train_sc.shape[1], activation='relu'))\n",
    "nn.add(Dense(2048, activation='relu'))\n",
    "\n",
    "\n",
    "# output layer\n",
    "nn.add(Dense(11, activation='softmax'))\n",
    "\n",
    "nn.compile(loss ='categorical_crossentropy', optimizer= 'adam', metrics=['acc'])\n",
    "\n",
    "results = nn.fit(X_train_sc, y_train,\n",
    "                    batch_size=256,\n",
    "                    epochs=32,\n",
    "                    validation_data=(X_test_sc,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2ee283bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 2.1795 - acc: 0.1874 - val_loss: 2.1068 - val_acc: 0.2201\n",
      "Epoch 2/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 2.0879 - acc: 0.2286 - val_loss: 2.0474 - val_acc: 0.2483\n",
      "Epoch 3/100\n",
      "624/624 [==============================] - 12s 20ms/step - loss: 2.0297 - acc: 0.2559 - val_loss: 1.9982 - val_acc: 0.2704\n",
      "Epoch 4/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.9675 - acc: 0.2846 - val_loss: 1.9509 - val_acc: 0.2928\n",
      "Epoch 5/100\n",
      "624/624 [==============================] - 12s 20ms/step - loss: 1.9137 - acc: 0.3076 - val_loss: 1.9009 - val_acc: 0.3122\n",
      "Epoch 6/100\n",
      "624/624 [==============================] - 12s 20ms/step - loss: 1.8611 - acc: 0.3276 - val_loss: 1.8768 - val_acc: 0.3240\n",
      "Epoch 7/100\n",
      "624/624 [==============================] - 12s 20ms/step - loss: 1.8166 - acc: 0.3458 - val_loss: 1.8477 - val_acc: 0.3349\n",
      "Epoch 8/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.7787 - acc: 0.3608 - val_loss: 1.8353 - val_acc: 0.3414\n",
      "Epoch 9/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.7431 - acc: 0.3744 - val_loss: 1.8108 - val_acc: 0.3525\n",
      "Epoch 10/100\n",
      "624/624 [==============================] - 12s 20ms/step - loss: 1.7119 - acc: 0.3862 - val_loss: 1.7951 - val_acc: 0.3579\n",
      "Epoch 11/100\n",
      "624/624 [==============================] - 12s 20ms/step - loss: 1.6851 - acc: 0.3961 - val_loss: 1.7857 - val_acc: 0.3621\n",
      "Epoch 12/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.6591 - acc: 0.4058 - val_loss: 1.7855 - val_acc: 0.3630\n",
      "Epoch 13/100\n",
      "624/624 [==============================] - 12s 20ms/step - loss: 1.6370 - acc: 0.4149 - val_loss: 1.7733 - val_acc: 0.3703\n",
      "Epoch 14/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.6123 - acc: 0.4237 - val_loss: 1.7650 - val_acc: 0.3735\n",
      "Epoch 15/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.5889 - acc: 0.4311 - val_loss: 1.7619 - val_acc: 0.3754\n",
      "Epoch 16/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.5679 - acc: 0.4401 - val_loss: 1.7529 - val_acc: 0.3795\n",
      "Epoch 17/100\n",
      "624/624 [==============================] - 12s 20ms/step - loss: 1.5487 - acc: 0.4470 - val_loss: 1.7589 - val_acc: 0.3801\n",
      "Epoch 18/100\n",
      "624/624 [==============================] - 12s 20ms/step - loss: 1.5313 - acc: 0.4528 - val_loss: 1.7614 - val_acc: 0.3806\n",
      "Epoch 19/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.5119 - acc: 0.4587 - val_loss: 1.7593 - val_acc: 0.3841\n",
      "Epoch 20/100\n",
      "624/624 [==============================] - 12s 20ms/step - loss: 1.4953 - acc: 0.4659 - val_loss: 1.7584 - val_acc: 0.3852\n",
      "Epoch 21/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.4798 - acc: 0.4715 - val_loss: 1.7665 - val_acc: 0.3835\n",
      "Epoch 22/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.4642 - acc: 0.4774 - val_loss: 1.7727 - val_acc: 0.3815\n",
      "Epoch 23/100\n",
      "624/624 [==============================] - 13s 20ms/step - loss: 1.4497 - acc: 0.4831 - val_loss: 1.7708 - val_acc: 0.3865\n",
      "Epoch 24/100\n",
      "624/624 [==============================] - 12s 20ms/step - loss: 1.4363 - acc: 0.4865 - val_loss: 1.7670 - val_acc: 0.3883\n",
      "Epoch 25/100\n",
      "624/624 [==============================] - 13s 21ms/step - loss: 1.4207 - acc: 0.4931 - val_loss: 1.7797 - val_acc: 0.3887\n",
      "Epoch 26/100\n",
      "624/624 [==============================] - 13s 20ms/step - loss: 1.4045 - acc: 0.4985 - val_loss: 1.7810 - val_acc: 0.3892\n",
      "Epoch 27/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.3957 - acc: 0.5016 - val_loss: 1.7983 - val_acc: 0.3877\n",
      "Epoch 28/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.3818 - acc: 0.5064 - val_loss: 1.7952 - val_acc: 0.3881\n",
      "Epoch 29/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.3681 - acc: 0.5119 - val_loss: 1.8067 - val_acc: 0.3861\n",
      "Epoch 30/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.3578 - acc: 0.5154 - val_loss: 1.8167 - val_acc: 0.3891\n",
      "Epoch 31/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.3448 - acc: 0.5203 - val_loss: 1.8160 - val_acc: 0.3898\n",
      "Epoch 32/100\n",
      "624/624 [==============================] - 12s 20ms/step - loss: 1.3319 - acc: 0.5251 - val_loss: 1.8201 - val_acc: 0.3891\n",
      "Epoch 33/100\n",
      "624/624 [==============================] - 13s 21ms/step - loss: 1.3231 - acc: 0.5270 - val_loss: 1.8309 - val_acc: 0.3892\n",
      "Epoch 34/100\n",
      "624/624 [==============================] - 13s 20ms/step - loss: 1.3126 - acc: 0.5316 - val_loss: 1.8412 - val_acc: 0.3888\n",
      "Epoch 35/100\n",
      "624/624 [==============================] - 11s 18ms/step - loss: 1.2996 - acc: 0.5361 - val_loss: 1.8463 - val_acc: 0.3897\n",
      "Epoch 36/100\n",
      "624/624 [==============================] - 10s 17ms/step - loss: 1.2918 - acc: 0.5390 - val_loss: 1.8543 - val_acc: 0.3903\n",
      "Epoch 37/100\n",
      "624/624 [==============================] - 11s 18ms/step - loss: 1.2812 - acc: 0.5427 - val_loss: 1.8618 - val_acc: 0.3900\n",
      "Epoch 38/100\n",
      "624/624 [==============================] - 11s 17ms/step - loss: 1.2700 - acc: 0.5461 - val_loss: 1.8744 - val_acc: 0.3893\n",
      "Epoch 39/100\n",
      "624/624 [==============================] - 11s 17ms/step - loss: 1.2622 - acc: 0.5495 - val_loss: 1.8792 - val_acc: 0.3895\n",
      "Epoch 40/100\n",
      "624/624 [==============================] - 11s 17ms/step - loss: 1.2566 - acc: 0.5514 - val_loss: 1.8861 - val_acc: 0.3904\n",
      "Epoch 41/100\n",
      "624/624 [==============================] - 11s 18ms/step - loss: 1.2448 - acc: 0.5566 - val_loss: 1.9139 - val_acc: 0.3890\n",
      "Epoch 42/100\n",
      "624/624 [==============================] - 12s 20ms/step - loss: 1.2372 - acc: 0.5588 - val_loss: 1.9087 - val_acc: 0.3893\n",
      "Epoch 43/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.2294 - acc: 0.5612 - val_loss: 1.9264 - val_acc: 0.3893\n",
      "Epoch 44/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.2202 - acc: 0.5643 - val_loss: 1.9411 - val_acc: 0.3886\n",
      "Epoch 45/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.2119 - acc: 0.5674 - val_loss: 1.9340 - val_acc: 0.3892\n",
      "Epoch 46/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.2027 - acc: 0.5708 - val_loss: 1.9420 - val_acc: 0.3885\n",
      "Epoch 47/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.1955 - acc: 0.5746 - val_loss: 1.9547 - val_acc: 0.3884\n",
      "Epoch 48/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.1886 - acc: 0.5748 - val_loss: 1.9571 - val_acc: 0.3873\n",
      "Epoch 49/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.1791 - acc: 0.5797 - val_loss: 1.9606 - val_acc: 0.3896\n",
      "Epoch 50/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.1736 - acc: 0.5804 - val_loss: 1.9906 - val_acc: 0.3883\n",
      "Epoch 51/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.1688 - acc: 0.5829 - val_loss: 1.9919 - val_acc: 0.3883\n",
      "Epoch 52/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.1609 - acc: 0.5866 - val_loss: 2.0013 - val_acc: 0.3871\n",
      "Epoch 53/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.1528 - acc: 0.5892 - val_loss: 2.0084 - val_acc: 0.3863\n",
      "Epoch 54/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.1476 - acc: 0.5906 - val_loss: 2.0179 - val_acc: 0.3854\n",
      "Epoch 55/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.1381 - acc: 0.5932 - val_loss: 2.0201 - val_acc: 0.3887\n",
      "Epoch 56/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.1332 - acc: 0.5961 - val_loss: 2.0303 - val_acc: 0.3865\n",
      "Epoch 57/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.1310 - acc: 0.5965 - val_loss: 2.0417 - val_acc: 0.3870\n",
      "Epoch 58/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.1259 - acc: 0.5983 - val_loss: 2.0598 - val_acc: 0.3887\n",
      "Epoch 59/100\n",
      "624/624 [==============================] - 11s 18ms/step - loss: 1.1148 - acc: 0.6021 - val_loss: 2.0660 - val_acc: 0.3871\n",
      "Epoch 60/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.1103 - acc: 0.6035 - val_loss: 2.0698 - val_acc: 0.3881\n",
      "Epoch 61/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.1069 - acc: 0.6049 - val_loss: 2.0872 - val_acc: 0.3880\n",
      "Epoch 62/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.0997 - acc: 0.6074 - val_loss: 2.0973 - val_acc: 0.3860\n",
      "Epoch 63/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.0987 - acc: 0.6073 - val_loss: 2.1032 - val_acc: 0.3858\n",
      "Epoch 64/100\n",
      "624/624 [==============================] - 12s 20ms/step - loss: 1.0925 - acc: 0.6108 - val_loss: 2.1287 - val_acc: 0.3854\n",
      "Epoch 65/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.0858 - acc: 0.6119 - val_loss: 2.1228 - val_acc: 0.3856\n",
      "Epoch 66/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.0795 - acc: 0.6146 - val_loss: 2.1295 - val_acc: 0.3868\n",
      "Epoch 67/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.0713 - acc: 0.6179 - val_loss: 2.1378 - val_acc: 0.3870\n",
      "Epoch 68/100\n",
      "624/624 [==============================] - 13s 21ms/step - loss: 1.0737 - acc: 0.6166 - val_loss: 2.1587 - val_acc: 0.3853\n",
      "Epoch 69/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.0636 - acc: 0.6204 - val_loss: 2.1598 - val_acc: 0.3832\n",
      "Epoch 70/100\n",
      "624/624 [==============================] - 11s 18ms/step - loss: 1.0582 - acc: 0.6225 - val_loss: 2.1716 - val_acc: 0.3845\n",
      "Epoch 71/100\n",
      "624/624 [==============================] - 11s 18ms/step - loss: 1.0562 - acc: 0.6233 - val_loss: 2.1799 - val_acc: 0.3842\n",
      "Epoch 72/100\n",
      "624/624 [==============================] - 12s 18ms/step - loss: 1.0504 - acc: 0.6247 - val_loss: 2.1910 - val_acc: 0.3839\n",
      "Epoch 73/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.0452 - acc: 0.6270 - val_loss: 2.2016 - val_acc: 0.3845\n",
      "Epoch 74/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.0430 - acc: 0.6271 - val_loss: 2.1968 - val_acc: 0.3851\n",
      "Epoch 75/100\n",
      "624/624 [==============================] - 12s 19ms/step - loss: 1.0407 - acc: 0.6279 - val_loss: 2.2108 - val_acc: 0.3843\n",
      "Epoch 76/100\n",
      "624/624 [==============================] - 13s 20ms/step - loss: 1.0332 - acc: 0.6313 - val_loss: 2.2303 - val_acc: 0.3848\n",
      "Epoch 77/100\n",
      "624/624 [==============================] - 13s 21ms/step - loss: 1.0290 - acc: 0.6322 - val_loss: 2.2348 - val_acc: 0.3829\n",
      "Epoch 78/100\n",
      "624/624 [==============================] - 13s 21ms/step - loss: 1.0238 - acc: 0.6351 - val_loss: 2.2477 - val_acc: 0.3827\n",
      "Epoch 79/100\n",
      "624/624 [==============================] - 13s 21ms/step - loss: 1.0216 - acc: 0.6350 - val_loss: 2.2593 - val_acc: 0.3811\n",
      "Epoch 80/100\n",
      "624/624 [==============================] - 13s 21ms/step - loss: 1.0192 - acc: 0.6361 - val_loss: 2.2614 - val_acc: 0.3829\n",
      "Epoch 81/100\n",
      "624/624 [==============================] - 13s 22ms/step - loss: 1.0112 - acc: 0.6396 - val_loss: 2.2966 - val_acc: 0.3859\n",
      "Epoch 82/100\n",
      "624/624 [==============================] - 13s 21ms/step - loss: 1.0086 - acc: 0.6398 - val_loss: 2.2934 - val_acc: 0.3821\n",
      "Epoch 83/100\n",
      "624/624 [==============================] - 13s 21ms/step - loss: 1.0069 - acc: 0.6402 - val_loss: 2.2892 - val_acc: 0.3828\n",
      "Epoch 84/100\n",
      "624/624 [==============================] - 13s 21ms/step - loss: 1.0012 - acc: 0.6424 - val_loss: 2.2994 - val_acc: 0.3815\n",
      "Epoch 85/100\n",
      "624/624 [==============================] - 13s 21ms/step - loss: 0.9955 - acc: 0.6451 - val_loss: 2.3296 - val_acc: 0.3820\n",
      "Epoch 86/100\n",
      "624/624 [==============================] - 13s 21ms/step - loss: 0.9921 - acc: 0.6464 - val_loss: 2.3156 - val_acc: 0.3833\n",
      "Epoch 87/100\n",
      "624/624 [==============================] - 13s 21ms/step - loss: 0.9901 - acc: 0.6459 - val_loss: 2.3229 - val_acc: 0.3801\n",
      "Epoch 88/100\n",
      "624/624 [==============================] - 13s 21ms/step - loss: 0.9859 - acc: 0.6483 - val_loss: 2.3322 - val_acc: 0.3814\n",
      "Epoch 89/100\n",
      "624/624 [==============================] - 13s 21ms/step - loss: 0.9844 - acc: 0.6486 - val_loss: 2.3590 - val_acc: 0.3805\n",
      "Epoch 90/100\n",
      "624/624 [==============================] - 13s 21ms/step - loss: 0.9833 - acc: 0.6486 - val_loss: 2.3437 - val_acc: 0.3823\n",
      "Epoch 91/100\n",
      "624/624 [==============================] - 13s 21ms/step - loss: 0.9765 - acc: 0.6509 - val_loss: 2.3661 - val_acc: 0.3817\n",
      "Epoch 92/100\n",
      "624/624 [==============================] - 13s 21ms/step - loss: 0.9732 - acc: 0.6523 - val_loss: 2.3778 - val_acc: 0.3810\n",
      "Epoch 93/100\n",
      "624/624 [==============================] - 13s 21ms/step - loss: 0.9703 - acc: 0.6533 - val_loss: 2.3728 - val_acc: 0.3813\n",
      "Epoch 94/100\n",
      "624/624 [==============================] - 13s 21ms/step - loss: 0.9647 - acc: 0.6552 - val_loss: 2.3996 - val_acc: 0.3817\n",
      "Epoch 95/100\n",
      "624/624 [==============================] - 13s 21ms/step - loss: 0.9659 - acc: 0.6549 - val_loss: 2.4121 - val_acc: 0.3831\n",
      "Epoch 96/100\n",
      "624/624 [==============================] - 13s 21ms/step - loss: 0.9602 - acc: 0.6572 - val_loss: 2.4176 - val_acc: 0.3813\n",
      "Epoch 97/100\n",
      "624/624 [==============================] - 13s 21ms/step - loss: 0.9551 - acc: 0.6598 - val_loss: 2.4193 - val_acc: 0.3820\n",
      "Epoch 98/100\n",
      "624/624 [==============================] - 13s 21ms/step - loss: 0.9535 - acc: 0.6591 - val_loss: 2.4226 - val_acc: 0.3811\n",
      "Epoch 99/100\n",
      "624/624 [==============================] - 13s 21ms/step - loss: 0.9501 - acc: 0.6603 - val_loss: 2.4490 - val_acc: 0.3814\n",
      "Epoch 100/100\n",
      "624/624 [==============================] - 13s 21ms/step - loss: 0.9490 - acc: 0.6604 - val_loss: 2.4368 - val_acc: 0.3782\n"
     ]
    }
   ],
   "source": [
    "# let's create our model\n",
    "nn= Sequential()\n",
    "# 1st hidden layer\n",
    "nn.add(Dense(512, input_dim=X_train_sc.shape[1], activation='relu'))\n",
    "nn.add(Dense(512, activation='relu'))\n",
    "nn.add(Dense(512, activation='relu'))\n",
    "nn.add(Dense(512, activation='relu'))\n",
    "\n",
    "# output layer\n",
    "nn.add(Dense(11, activation='softmax'))\n",
    "\n",
    "nn.compile(loss ='categorical_crossentropy', optimizer= 'adam', metrics=['acc'])\n",
    "\n",
    "results = nn.fit(X_train_sc, y_train,\n",
    "                    batch_size=1024,\n",
    "                    epochs=100,\n",
    "                    validation_data=(X_test_sc,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "93e2f6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca=PCA()\n",
    "pca.fit(X_train_sc)\n",
    "Z_train =pca.transform(X_train_sc)\n",
    "Z_test = pca.transform(X_test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ca802fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance (first 20 components): [0.29250123 0.24555788 0.13175641 0.08162141 0.05747355 0.05360277\n",
      " 0.04149689 0.03025271 0.02440467 0.01465115 0.00801879 0.00768075\n",
      " 0.00609225 0.00245149 0.00191975 0.00051829]\n"
     ]
    }
   ],
   "source": [
    "var_exp =pca.explained_variance_ratio_\n",
    "print(f'Explained variance (first 20 components): {var_exp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1635606b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance (first 20 components): [0.29250123 0.24555788 0.13175641 0.08162141 0.05747355 0.05360277\n",
      " 0.04149689 0.03025271 0.02440467 0.01465115]\n"
     ]
    }
   ],
   "source": [
    "pca=PCA(n_components=10)\n",
    "pca.fit(X_train_sc)\n",
    "Z_train =pca.transform(X_train_sc)\n",
    "Z_test = pca.transform(X_test_sc)\n",
    "var_exp =pca.explained_variance_ratio_\n",
    "print(f'Explained variance (first 20 components): {var_exp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0c3fde21",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "2495/2495 [==============================] - 14s 5ms/step - loss: 2.1823 - acc: 0.1870 - val_loss: 2.1321 - val_acc: 0.2078\n",
      "Epoch 2/25\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.1191 - acc: 0.2133 - val_loss: 2.0980 - val_acc: 0.2241\n",
      "Epoch 3/25\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.0824 - acc: 0.2306 - val_loss: 2.0716 - val_acc: 0.2362\n",
      "Epoch 4/25\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.0497 - acc: 0.2455 - val_loss: 2.0440 - val_acc: 0.2487\n",
      "Epoch 5/25\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.0237 - acc: 0.2578 - val_loss: 2.0259 - val_acc: 0.2571\n",
      "Epoch 6/25\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.0012 - acc: 0.2663 - val_loss: 2.0162 - val_acc: 0.2626\n",
      "Epoch 7/25\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.9810 - acc: 0.2748 - val_loss: 2.0026 - val_acc: 0.2670\n",
      "Epoch 8/25\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.9618 - acc: 0.2824 - val_loss: 1.9958 - val_acc: 0.2703\n",
      "Epoch 9/25\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.9450 - acc: 0.2890 - val_loss: 1.9889 - val_acc: 0.2740\n",
      "Epoch 10/25\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.9327 - acc: 0.2933 - val_loss: 1.9840 - val_acc: 0.2760\n",
      "Epoch 11/25\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.9173 - acc: 0.3007 - val_loss: 1.9747 - val_acc: 0.2808\n",
      "Epoch 12/25\n",
      "2495/2495 [==============================] - 14s 5ms/step - loss: 1.9040 - acc: 0.3061 - val_loss: 1.9671 - val_acc: 0.2826\n",
      "Epoch 13/25\n",
      "2495/2495 [==============================] - 14s 5ms/step - loss: 1.8933 - acc: 0.3106 - val_loss: 1.9665 - val_acc: 0.2844\n",
      "Epoch 14/25\n",
      "2495/2495 [==============================] - 14s 6ms/step - loss: 1.8812 - acc: 0.3155 - val_loss: 1.9620 - val_acc: 0.2875\n",
      "Epoch 15/25\n",
      "2495/2495 [==============================] - 14s 6ms/step - loss: 1.8713 - acc: 0.3187 - val_loss: 1.9585 - val_acc: 0.2885\n",
      "Epoch 16/25\n",
      "2495/2495 [==============================] - 14s 6ms/step - loss: 1.8609 - acc: 0.3222 - val_loss: 1.9603 - val_acc: 0.2884\n",
      "Epoch 17/25\n",
      "2495/2495 [==============================] - 14s 6ms/step - loss: 1.8490 - acc: 0.3261 - val_loss: 1.9514 - val_acc: 0.2923\n",
      "Epoch 18/25\n",
      "2495/2495 [==============================] - 14s 5ms/step - loss: 1.8406 - acc: 0.3302 - val_loss: 1.9508 - val_acc: 0.2936\n",
      "Epoch 19/25\n",
      "2495/2495 [==============================] - 14s 6ms/step - loss: 1.8327 - acc: 0.3329 - val_loss: 1.9522 - val_acc: 0.2942\n",
      "Epoch 20/25\n",
      "2495/2495 [==============================] - 14s 5ms/step - loss: 1.8247 - acc: 0.3362 - val_loss: 1.9526 - val_acc: 0.2962\n",
      "Epoch 21/25\n",
      "2495/2495 [==============================] - 14s 6ms/step - loss: 1.8128 - acc: 0.3412 - val_loss: 1.9452 - val_acc: 0.2970\n",
      "Epoch 22/25\n",
      "2495/2495 [==============================] - 14s 6ms/step - loss: 1.8072 - acc: 0.3430 - val_loss: 1.9417 - val_acc: 0.2983\n",
      "Epoch 23/25\n",
      "2495/2495 [==============================] - 14s 5ms/step - loss: 1.7980 - acc: 0.3474 - val_loss: 1.9498 - val_acc: 0.2992\n",
      "Epoch 24/25\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.7896 - acc: 0.3505 - val_loss: 1.9481 - val_acc: 0.3000\n",
      "Epoch 25/25\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.7826 - acc: 0.3527 - val_loss: 1.9414 - val_acc: 0.3003\n"
     ]
    }
   ],
   "source": [
    "# let's create our model\n",
    "nn= Sequential()\n",
    "# 1st hidden layer\n",
    "nn.add(Dense(512, input_dim=Z_train.shape[1], activation='relu'))\n",
    "nn.add(Dense(512, activation='relu'))\n",
    "nn.add(Dense(512, activation='relu'))\n",
    "\n",
    "\n",
    "# output layer\n",
    "nn.add(Dense(11, activation='softmax'))\n",
    "\n",
    "nn.compile(loss ='categorical_crossentropy', optimizer= 'adam', metrics=['acc'])\n",
    "\n",
    "results1 = nn.fit(Z_train, y_train,\n",
    "                    batch_size=256,\n",
    "                    epochs=25,\n",
    "                    validation_data=(Z_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a8f26c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just using temp with best params found\n",
    "X= df[['Tx','Tn','Tm']]\n",
    "y = df['decade']\n",
    "\n",
    "y = to_categorical(y)\n",
    "X_train_t, X_test_t,y_train_t,y_test_t = train_test_split(X,y, stratify=y)\n",
    "\n",
    "sc_t = StandardScaler()\n",
    "\n",
    "X_train_t_sc = sc_t.fit_transform(X_train_t)\n",
    "X_test_t_sc = sc_t.transform(X_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "64a0e8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.2667 - acc: 0.1505 - val_loss: 2.2582 - val_acc: 0.1565\n",
      "Epoch 2/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.2564 - acc: 0.1567 - val_loss: 2.2532 - val_acc: 0.1616\n",
      "Epoch 3/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.2525 - acc: 0.1604 - val_loss: 2.2501 - val_acc: 0.1631\n",
      "Epoch 4/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.2498 - acc: 0.1624 - val_loss: 2.2490 - val_acc: 0.1647\n",
      "Epoch 5/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.2480 - acc: 0.1643 - val_loss: 2.2478 - val_acc: 0.1655\n",
      "Epoch 6/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.2461 - acc: 0.1656 - val_loss: 2.2459 - val_acc: 0.1663\n",
      "Epoch 7/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.2439 - acc: 0.1665 - val_loss: 2.2451 - val_acc: 0.1663\n",
      "Epoch 8/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.2435 - acc: 0.1678 - val_loss: 2.2443 - val_acc: 0.1677\n",
      "Epoch 9/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.2432 - acc: 0.1694 - val_loss: 2.2429 - val_acc: 0.1678\n",
      "Epoch 10/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.2434 - acc: 0.1695 - val_loss: 2.2435 - val_acc: 0.1678\n",
      "Epoch 11/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.2418 - acc: 0.1690 - val_loss: 2.2424 - val_acc: 0.1699\n",
      "Epoch 12/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.2399 - acc: 0.1702 - val_loss: 2.2419 - val_acc: 0.1697\n",
      "Epoch 13/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.2397 - acc: 0.1712 - val_loss: 2.2419 - val_acc: 0.1680\n",
      "Epoch 14/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.2402 - acc: 0.1700 - val_loss: 2.2418 - val_acc: 0.1696\n",
      "Epoch 15/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.2395 - acc: 0.1702 - val_loss: 2.2414 - val_acc: 0.1689\n",
      "Epoch 16/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.2382 - acc: 0.1717 - val_loss: 2.2420 - val_acc: 0.1687\n"
     ]
    }
   ],
   "source": [
    "# let's create our model\n",
    "nn_t= Sequential()\n",
    "# 1st hidden layer\n",
    "nn_t.add(Dense(512, input_dim=X_train_t_sc.shape[1], activation='relu'))\n",
    "nn_t.add(Dense(512, activation='relu'))\n",
    "nn_t.add(Dense(512, activation='relu'))\n",
    "\n",
    "# output layer\n",
    "nn_t.add(Dense(11, activation='softmax'))\n",
    "\n",
    "nn_t.compile(loss ='categorical_crossentropy', optimizer= 'adam', metrics=['acc'])\n",
    "\n",
    "results_t = nn_t.fit(X_train_t_sc, y_train_t,\n",
    "                    batch_size=256,\n",
    "                    epochs=16,\n",
    "                    validation_data=(X_test_t_sc,y_test_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4c5d6b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just using temp, logitude and latitude with best params found\n",
    "X= df[['Tx','Tn','Tm', 'Long', 'Lat']]\n",
    "y = df['decade']\n",
    "\n",
    "y = to_categorical(y)\n",
    "X_train_t, X_test_t,y_train_t,y_test_t = train_test_split(X,y, stratify=y)\n",
    "\n",
    "sc_t = StandardScaler()\n",
    "\n",
    "X_train_t_sc = sc_t.fit_transform(X_train_t)\n",
    "X_test_t_sc = sc_t.transform(X_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4612434f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.2562 - acc: 0.1536 - val_loss: 2.2277 - val_acc: 0.1704\n",
      "Epoch 2/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.2213 - acc: 0.1722 - val_loss: 2.2066 - val_acc: 0.1809\n",
      "Epoch 3/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.1999 - acc: 0.1859 - val_loss: 2.1924 - val_acc: 0.1887\n",
      "Epoch 4/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.1826 - acc: 0.1930 - val_loss: 2.1746 - val_acc: 0.1977\n",
      "Epoch 5/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.1661 - acc: 0.2014 - val_loss: 2.1661 - val_acc: 0.2015\n",
      "Epoch 6/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.1534 - acc: 0.2070 - val_loss: 2.1560 - val_acc: 0.2023\n",
      "Epoch 7/16\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 2.1429 - acc: 0.2113 - val_loss: 2.1478 - val_acc: 0.2068\n",
      "Epoch 8/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.1322 - acc: 0.2150 - val_loss: 2.1399 - val_acc: 0.2098\n",
      "Epoch 9/16\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 2.1232 - acc: 0.2169 - val_loss: 2.1367 - val_acc: 0.2125\n",
      "Epoch 10/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.1138 - acc: 0.2209 - val_loss: 2.1312 - val_acc: 0.2148\n",
      "Epoch 11/16\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 2.1052 - acc: 0.2244 - val_loss: 2.1245 - val_acc: 0.2147\n",
      "Epoch 12/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.0986 - acc: 0.2262 - val_loss: 2.1207 - val_acc: 0.2181\n",
      "Epoch 13/16\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 2.0933 - acc: 0.2302 - val_loss: 2.1188 - val_acc: 0.2171\n",
      "Epoch 14/16\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 2.0876 - acc: 0.2291 - val_loss: 2.1156 - val_acc: 0.2180\n",
      "Epoch 15/16\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 2.0801 - acc: 0.2327 - val_loss: 2.1087 - val_acc: 0.2198\n",
      "Epoch 16/16\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.0725 - acc: 0.2363 - val_loss: 2.1038 - val_acc: 0.2226\n"
     ]
    }
   ],
   "source": [
    "# let's create our model\n",
    "nn_t= Sequential()\n",
    "# 1st hidden layer\n",
    "nn_t.add(Dense(1024, input_dim=X_train_t_sc.shape[1], activation='relu'))\n",
    "\n",
    "nn_t.add(Dense(1024, activation='relu'))\n",
    "\n",
    "nn_t.add(Dense(1024, activation='relu'))\n",
    "\n",
    "# output layer\n",
    "nn_t.add(Dense(11, activation='softmax'))\n",
    "\n",
    "nn_t.compile(loss ='categorical_crossentropy', optimizer= 'adam', metrics=['acc'])\n",
    "\n",
    "results_t = nn_t.fit(X_train_t_sc, y_train_t,\n",
    "                    batch_size=256,\n",
    "                    epochs=16,\n",
    "                    validation_data=(X_test_t_sc,y_test_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "772a2399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2495/2495 [==============================] - 44s 18ms/step - loss: 2.2083 - acc: 0.1770 - val_loss: 2.1458 - val_acc: 0.2004\n",
      "Epoch 2/16\n",
      "2495/2495 [==============================] - 44s 18ms/step - loss: 2.1574 - acc: 0.1974 - val_loss: 2.1285 - val_acc: 0.2085\n",
      "Epoch 3/16\n",
      "2495/2495 [==============================] - 44s 18ms/step - loss: 2.1448 - acc: 0.2017 - val_loss: 2.1133 - val_acc: 0.2182\n",
      "Epoch 4/16\n",
      "2495/2495 [==============================] - 43s 17ms/step - loss: 2.1347 - acc: 0.2061 - val_loss: 2.1070 - val_acc: 0.2205\n",
      "Epoch 5/16\n",
      "2495/2495 [==============================] - 43s 17ms/step - loss: 2.1264 - acc: 0.2106 - val_loss: 2.0936 - val_acc: 0.2265\n",
      "Epoch 6/16\n",
      "2495/2495 [==============================] - 43s 17ms/step - loss: 2.1222 - acc: 0.2115 - val_loss: 2.0906 - val_acc: 0.2288\n",
      "Epoch 7/16\n",
      "2495/2495 [==============================] - 43s 17ms/step - loss: 2.1163 - acc: 0.2149 - val_loss: 2.0802 - val_acc: 0.2327\n",
      "Epoch 8/16\n",
      "2495/2495 [==============================] - 43s 17ms/step - loss: 2.1099 - acc: 0.2178 - val_loss: 2.0757 - val_acc: 0.2345\n",
      "Epoch 9/16\n",
      "2495/2495 [==============================] - 44s 18ms/step - loss: 2.1048 - acc: 0.2197 - val_loss: 2.0675 - val_acc: 0.2394\n",
      "Epoch 10/16\n",
      "2495/2495 [==============================] - 44s 18ms/step - loss: 2.1015 - acc: 0.2223 - val_loss: 2.0652 - val_acc: 0.2398\n",
      "Epoch 11/16\n",
      "2495/2495 [==============================] - 43s 17ms/step - loss: 2.0991 - acc: 0.2225 - val_loss: 2.0691 - val_acc: 0.2404\n",
      "Epoch 12/16\n",
      "2495/2495 [==============================] - 43s 17ms/step - loss: 2.0938 - acc: 0.2241 - val_loss: 2.0600 - val_acc: 0.2441\n",
      "Epoch 13/16\n",
      "2495/2495 [==============================] - 43s 17ms/step - loss: 2.0921 - acc: 0.2259 - val_loss: 2.0539 - val_acc: 0.2497\n",
      "Epoch 14/16\n",
      "2495/2495 [==============================] - 43s 17ms/step - loss: 2.0877 - acc: 0.2281 - val_loss: 2.0492 - val_acc: 0.2501\n",
      "Epoch 15/16\n",
      "2495/2495 [==============================] - 43s 17ms/step - loss: 2.0855 - acc: 0.2296 - val_loss: 2.0464 - val_acc: 0.2504\n",
      "Epoch 16/16\n",
      "2495/2495 [==============================] - 43s 17ms/step - loss: 2.0838 - acc: 0.2306 - val_loss: 2.0450 - val_acc: 0.2548\n"
     ]
    }
   ],
   "source": [
    "# let's create our model\n",
    "nn_dropout= Sequential()\n",
    "# 1st hidden layer\n",
    "nn_dropout.add(Dense(1024, input_dim=X_train_sc.shape[1], activation='relu'))\n",
    "\n",
    "nn_dropout.add(Dropout(.5))\n",
    "\n",
    "nn_dropout.add(Dense(1024, activation='relu'))\n",
    "\n",
    "nn_dropout.add(Dropout(.5))\n",
    "\n",
    "nn_dropout.add(Dense(1024, activation='relu'))\n",
    "\n",
    "nn_dropout.add(Dropout(.5))\n",
    "# output layer\n",
    "nn_dropout.add(Dense(11, activation='softmax'))\n",
    "\n",
    "nn_dropout.compile(loss ='categorical_crossentropy', optimizer= 'adam', metrics=['acc'])\n",
    "\n",
    "results1 = nn_dropout.fit(X_train_sc, y_train,\n",
    "                    batch_size=256,\n",
    "                    epochs=16,\n",
    "                    validation_data=(X_test_sc,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9c439c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "624/624 [==============================] - 8s 13ms/step - loss: 2.1812 - acc: 0.1884 - val_loss: 2.1140 - val_acc: 0.2182\n",
      "Epoch 2/20\n",
      "624/624 [==============================] - 8s 13ms/step - loss: 2.0990 - acc: 0.2241 - val_loss: 2.0671 - val_acc: 0.2404\n",
      "Epoch 3/20\n",
      "624/624 [==============================] - 8s 13ms/step - loss: 2.0544 - acc: 0.2445 - val_loss: 2.0300 - val_acc: 0.2553\n",
      "Epoch 4/20\n",
      "624/624 [==============================] - 7s 12ms/step - loss: 2.0122 - acc: 0.2644 - val_loss: 1.9956 - val_acc: 0.2721\n",
      "Epoch 5/20\n",
      "624/624 [==============================] - 8s 13ms/step - loss: 1.9713 - acc: 0.2831 - val_loss: 1.9610 - val_acc: 0.2894\n",
      "Epoch 6/20\n",
      "624/624 [==============================] - 7s 12ms/step - loss: 1.9340 - acc: 0.3009 - val_loss: 1.9405 - val_acc: 0.2982\n",
      "Epoch 7/20\n",
      "624/624 [==============================] - 7s 12ms/step - loss: 1.9018 - acc: 0.3139 - val_loss: 1.9186 - val_acc: 0.3066\n",
      "Epoch 8/20\n",
      "624/624 [==============================] - 7s 12ms/step - loss: 1.8742 - acc: 0.3245 - val_loss: 1.8981 - val_acc: 0.3167\n",
      "Epoch 9/20\n",
      "624/624 [==============================] - 7s 12ms/step - loss: 1.8478 - acc: 0.3351 - val_loss: 1.8819 - val_acc: 0.3240\n",
      "Epoch 10/20\n",
      "624/624 [==============================] - 7s 12ms/step - loss: 1.8238 - acc: 0.3449 - val_loss: 1.8624 - val_acc: 0.3297\n",
      "Epoch 11/20\n",
      "624/624 [==============================] - 7s 12ms/step - loss: 1.8022 - acc: 0.3528 - val_loss: 1.8538 - val_acc: 0.3343\n",
      "Epoch 12/20\n",
      "624/624 [==============================] - 7s 12ms/step - loss: 1.7815 - acc: 0.3615 - val_loss: 1.8455 - val_acc: 0.3392\n",
      "Epoch 13/20\n",
      "624/624 [==============================] - 7s 12ms/step - loss: 1.7662 - acc: 0.3667 - val_loss: 1.8382 - val_acc: 0.3413\n",
      "Epoch 14/20\n",
      "624/624 [==============================] - 7s 12ms/step - loss: 1.7447 - acc: 0.3753 - val_loss: 1.8318 - val_acc: 0.3436\n",
      "Epoch 15/20\n",
      "624/624 [==============================] - 7s 12ms/step - loss: 1.7289 - acc: 0.3816 - val_loss: 1.8218 - val_acc: 0.3504\n",
      "Epoch 16/20\n",
      "624/624 [==============================] - 7s 12ms/step - loss: 1.7178 - acc: 0.3857 - val_loss: 1.8175 - val_acc: 0.3518\n",
      "Epoch 17/20\n",
      "624/624 [==============================] - 7s 12ms/step - loss: 1.7028 - acc: 0.3912 - val_loss: 1.8125 - val_acc: 0.3521\n",
      "Epoch 18/20\n",
      "624/624 [==============================] - 7s 12ms/step - loss: 1.6909 - acc: 0.3950 - val_loss: 1.8018 - val_acc: 0.3584\n",
      "Epoch 19/20\n",
      "624/624 [==============================] - 7s 12ms/step - loss: 1.6751 - acc: 0.4008 - val_loss: 1.8062 - val_acc: 0.3578\n",
      "Epoch 20/20\n",
      "624/624 [==============================] - 7s 12ms/step - loss: 1.6645 - acc: 0.4054 - val_loss: 1.7967 - val_acc: 0.3598\n"
     ]
    }
   ],
   "source": [
    "# let's create our model\n",
    "nn= Sequential()\n",
    "# 1st hidden layer\n",
    "nn.add(Dense(512, input_dim=X_train_sc.shape[1], activation='relu'))\n",
    "nn.add(Dense(512, activation='relu'))\n",
    "nn.add(Dense(512, activation='relu'))\n",
    "\n",
    "\n",
    "# output layer\n",
    "nn.add(Dense(11, activation='softmax'))\n",
    "\n",
    "nn.compile(loss ='categorical_crossentropy', optimizer= 'adam', metrics=['acc'])\n",
    "\n",
    "results = nn.fit(X_train_sc, y_train,\n",
    "                    batch_size=1024,\n",
    "                    epochs=20,\n",
    "                    validation_data=(X_test_sc,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e744ce6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.1684 - acc: 0.1924 - val_loss: 2.0955 - val_acc: 0.2260\n",
      "Epoch 2/32\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 2.0781 - acc: 0.2321 - val_loss: 2.0333 - val_acc: 0.2552\n",
      "Epoch 3/32\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 2.0112 - acc: 0.2641 - val_loss: 1.9793 - val_acc: 0.2783\n",
      "Epoch 4/32\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 1.9553 - acc: 0.2889 - val_loss: 1.9430 - val_acc: 0.2948\n",
      "Epoch 5/32\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 1.9078 - acc: 0.3094 - val_loss: 1.9145 - val_acc: 0.3075\n",
      "Epoch 6/32\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 1.8655 - acc: 0.3269 - val_loss: 1.8870 - val_acc: 0.3188\n",
      "Epoch 7/32\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 1.8343 - acc: 0.3395 - val_loss: 1.8707 - val_acc: 0.3266\n",
      "Epoch 8/32\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 1.8033 - acc: 0.3514 - val_loss: 1.8503 - val_acc: 0.3350\n",
      "Epoch 9/32\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 1.7774 - acc: 0.3609 - val_loss: 1.8421 - val_acc: 0.3384\n",
      "Epoch 10/32\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 1.7567 - acc: 0.3687 - val_loss: 1.8348 - val_acc: 0.3426\n",
      "Epoch 11/32\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 1.7379 - acc: 0.3763 - val_loss: 1.8258 - val_acc: 0.3469\n",
      "Epoch 12/32\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 1.7182 - acc: 0.3829 - val_loss: 1.8214 - val_acc: 0.3496\n",
      "Epoch 13/32\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 1.7038 - acc: 0.3896 - val_loss: 1.8082 - val_acc: 0.3536\n",
      "Epoch 14/32\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 1.6878 - acc: 0.3948 - val_loss: 1.8044 - val_acc: 0.3578\n",
      "Epoch 15/32\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.6731 - acc: 0.4012 - val_loss: 1.8040 - val_acc: 0.3587\n",
      "Epoch 16/32\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 1.6588 - acc: 0.4043 - val_loss: 1.7997 - val_acc: 0.3600\n",
      "Epoch 17/32\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 1.6459 - acc: 0.4106 - val_loss: 1.7985 - val_acc: 0.3624\n",
      "Epoch 18/32\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.6362 - acc: 0.4138 - val_loss: 1.8013 - val_acc: 0.3626\n",
      "Epoch 19/32\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.6243 - acc: 0.4188 - val_loss: 1.7914 - val_acc: 0.3650\n",
      "Epoch 20/32\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.6136 - acc: 0.4225 - val_loss: 1.8017 - val_acc: 0.3651\n",
      "Epoch 21/32\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 1.6034 - acc: 0.4263 - val_loss: 1.7959 - val_acc: 0.3658\n",
      "Epoch 22/32\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 1.5973 - acc: 0.4289 - val_loss: 1.7941 - val_acc: 0.3669\n",
      "Epoch 23/32\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 1.5863 - acc: 0.4320 - val_loss: 1.7944 - val_acc: 0.3687\n",
      "Epoch 24/32\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 1.5750 - acc: 0.4354 - val_loss: 1.7898 - val_acc: 0.3706\n",
      "Epoch 25/32\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 1.5672 - acc: 0.4387 - val_loss: 1.7925 - val_acc: 0.3715\n",
      "Epoch 26/32\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.5599 - acc: 0.4419 - val_loss: 1.8011 - val_acc: 0.3704\n",
      "Epoch 27/32\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 1.5522 - acc: 0.4454 - val_loss: 1.7972 - val_acc: 0.3729\n",
      "Epoch 28/32\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 1.5434 - acc: 0.4479 - val_loss: 1.7916 - val_acc: 0.3735\n",
      "Epoch 29/32\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 1.5385 - acc: 0.4498 - val_loss: 1.8004 - val_acc: 0.3730\n",
      "Epoch 30/32\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 1.5344 - acc: 0.4517 - val_loss: 1.8081 - val_acc: 0.3710\n",
      "Epoch 31/32\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 1.5264 - acc: 0.4545 - val_loss: 1.8028 - val_acc: 0.3729\n",
      "Epoch 32/32\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 1.5178 - acc: 0.4579 - val_loss: 1.8147 - val_acc: 0.3731\n"
     ]
    }
   ],
   "source": [
    "# let's create our model\n",
    "nn1= Sequential()\n",
    "# 1st hidden layer\n",
    "nn1.add(Dense(512, input_dim=X_train_sc.shape[1], activation='relu'))\n",
    "nn1.add(Dense(512, activation='relu'))\n",
    "nn1.add(Dense(512, activation='relu'))\n",
    "\n",
    "# output layer\n",
    "nn1.add(Dense(11, activation='softmax'))\n",
    "\n",
    "nn1.compile(loss ='categorical_crossentropy', optimizer= 'adam', metrics=['acc'])\n",
    "\n",
    "results1 = nn1.fit(X_train_sc, y_train,\n",
    "                    batch_size=256,\n",
    "                    epochs=32,\n",
    "                    validation_data=(X_test_sc,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "53abeefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "9978/9978 [==============================] - 35s 3ms/step - loss: 2.1672 - acc: 0.1923 - val_loss: 2.0904 - val_acc: 0.2263\n",
      "Epoch 2/30\n",
      "9978/9978 [==============================] - 35s 4ms/step - loss: 2.0699 - acc: 0.2362 - val_loss: 2.0236 - val_acc: 0.2572\n",
      "Epoch 3/30\n",
      "9978/9978 [==============================] - 35s 4ms/step - loss: 1.9973 - acc: 0.2701 - val_loss: 1.9688 - val_acc: 0.2828\n",
      "Epoch 4/30\n",
      "9978/9978 [==============================] - 35s 4ms/step - loss: 1.9466 - acc: 0.2917 - val_loss: 1.9409 - val_acc: 0.2956\n",
      "Epoch 5/30\n",
      "9978/9978 [==============================] - 35s 4ms/step - loss: 1.9048 - acc: 0.3101 - val_loss: 1.9189 - val_acc: 0.3046\n",
      "Epoch 6/30\n",
      "9978/9978 [==============================] - 35s 4ms/step - loss: 1.8771 - acc: 0.3212 - val_loss: 1.8938 - val_acc: 0.3158\n",
      "Epoch 7/30\n",
      "9978/9978 [==============================] - 35s 4ms/step - loss: 1.8518 - acc: 0.3308 - val_loss: 1.8873 - val_acc: 0.3190\n",
      "Epoch 8/30\n",
      "9978/9978 [==============================] - 35s 3ms/step - loss: 1.8294 - acc: 0.3399 - val_loss: 1.8744 - val_acc: 0.3239\n",
      "Epoch 9/30\n",
      "9978/9978 [==============================] - 35s 4ms/step - loss: 1.8108 - acc: 0.3466 - val_loss: 1.8641 - val_acc: 0.3299\n",
      "Epoch 10/30\n",
      "9978/9978 [==============================] - 35s 4ms/step - loss: 1.7951 - acc: 0.3517 - val_loss: 1.8578 - val_acc: 0.3321\n",
      "Epoch 11/30\n",
      "9978/9978 [==============================] - 35s 4ms/step - loss: 1.7812 - acc: 0.3584 - val_loss: 1.8519 - val_acc: 0.3348\n",
      "Epoch 12/30\n",
      "9978/9978 [==============================] - 35s 4ms/step - loss: 1.7688 - acc: 0.3631 - val_loss: 1.8534 - val_acc: 0.3366\n",
      "Epoch 13/30\n",
      "9978/9978 [==============================] - 35s 4ms/step - loss: 1.7579 - acc: 0.3678 - val_loss: 1.8508 - val_acc: 0.3406\n",
      "Epoch 14/30\n",
      "9978/9978 [==============================] - 35s 4ms/step - loss: 1.7474 - acc: 0.3715 - val_loss: 1.8457 - val_acc: 0.3405\n",
      "Epoch 15/30\n",
      "9978/9978 [==============================] - 35s 4ms/step - loss: 1.7379 - acc: 0.3747 - val_loss: 1.8406 - val_acc: 0.3437\n",
      "Epoch 16/30\n",
      "9978/9978 [==============================] - 35s 3ms/step - loss: 1.7277 - acc: 0.3787 - val_loss: 1.8386 - val_acc: 0.3432\n",
      "Epoch 17/30\n",
      "9978/9978 [==============================] - 35s 3ms/step - loss: 1.7200 - acc: 0.3825 - val_loss: 1.8420 - val_acc: 0.3432\n",
      "Epoch 18/30\n",
      "9978/9978 [==============================] - 35s 4ms/step - loss: 1.7104 - acc: 0.3862 - val_loss: 1.8419 - val_acc: 0.3462\n",
      "Epoch 19/30\n",
      "9978/9978 [==============================] - 35s 3ms/step - loss: 1.7002 - acc: 0.3894 - val_loss: 1.8372 - val_acc: 0.3459\n",
      "Epoch 20/30\n",
      "9978/9978 [==============================] - 35s 4ms/step - loss: 1.6936 - acc: 0.3915 - val_loss: 1.8376 - val_acc: 0.3479\n",
      "Epoch 21/30\n",
      "9978/9978 [==============================] - 35s 4ms/step - loss: 1.6896 - acc: 0.3933 - val_loss: 1.8373 - val_acc: 0.3486\n",
      "Epoch 22/30\n",
      "9978/9978 [==============================] - 35s 4ms/step - loss: 1.6835 - acc: 0.3950 - val_loss: 1.8325 - val_acc: 0.3509\n",
      "Epoch 23/30\n",
      "9978/9978 [==============================] - 35s 3ms/step - loss: 1.6773 - acc: 0.3967 - val_loss: 1.8314 - val_acc: 0.3511\n",
      "Epoch 24/30\n",
      "9978/9978 [==============================] - 35s 3ms/step - loss: 1.6707 - acc: 0.3996 - val_loss: 1.8327 - val_acc: 0.3515\n",
      "Epoch 25/30\n",
      "9978/9978 [==============================] - 35s 4ms/step - loss: 1.6659 - acc: 0.4024 - val_loss: 1.8354 - val_acc: 0.3524\n",
      "Epoch 26/30\n",
      "9978/9978 [==============================] - 35s 3ms/step - loss: 1.6578 - acc: 0.4053 - val_loss: 1.8383 - val_acc: 0.3524\n",
      "Epoch 27/30\n",
      "9978/9978 [==============================] - 35s 4ms/step - loss: 1.6537 - acc: 0.4065 - val_loss: 1.8438 - val_acc: 0.3533\n",
      "Epoch 28/30\n",
      "9978/9978 [==============================] - 35s 3ms/step - loss: 1.6463 - acc: 0.4085 - val_loss: 1.8324 - val_acc: 0.3548\n",
      "Epoch 29/30\n",
      "9978/9978 [==============================] - 35s 4ms/step - loss: 1.6432 - acc: 0.4100 - val_loss: 1.8412 - val_acc: 0.3557\n",
      "Epoch 30/30\n",
      "9978/9978 [==============================] - 35s 4ms/step - loss: 1.6397 - acc: 0.4115 - val_loss: 1.8411 - val_acc: 0.3566\n"
     ]
    }
   ],
   "source": [
    "# let's create our model\n",
    "nn2= Sequential()\n",
    "# 1st hidden layer\n",
    "nn2.add(Dense(512, input_dim=X_train_sc.shape[1], activation='relu'))\n",
    "nn2.add(Dense(512, activation='relu'))\n",
    "nn2.add(Dense(512, activation='relu'))\n",
    "\n",
    "# output layer\n",
    "nn2.add(Dense(11, activation='softmax'))\n",
    "\n",
    "nn2.compile(loss ='categorical_crossentropy', optimizer= 'adam', metrics=['acc'])\n",
    "\n",
    "results2 = nn2.fit(X_train_sc, y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=30,\n",
    "                    validation_data=(X_test_sc,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4a9a6b09",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot pickle 'weakref' object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-110-303968723765>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'store'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'nn2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2342\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2343\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2344\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2345\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\decorator.py\u001b[0m in \u001b[0;36mfun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    229\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m     \u001b[0mfun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\extensions\\storemagic.py\u001b[0m in \u001b[0;36mstore\u001b[1;34m(self, parameter_s)\u001b[0m\n\u001b[0;32m    224\u001b[0m                         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                     \u001b[1;31m#pickled = pickle.dumps(obj)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m                     \u001b[0mdb\u001b[0m\u001b[1;33m[\u001b[0m \u001b[1;34m'autorestore/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0marg\u001b[0m \u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Stored '%s' (%s)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pickleshare.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;31m# and Python 3. We can upgrade to protocol 3 when Python 2 is obsolete.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mfil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m             \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfil\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mst_mtime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot pickle 'weakref' object"
     ]
    }
   ],
   "source": [
    "%store nn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8857b9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LxL'] = df['Long'] * df['Lat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1e1fd195",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= df.drop(columns= ['Stn_Name', 'Prov', 'Clim_ID', 'decade', 'Year'])\n",
    "y = df['decade']\n",
    "\n",
    "y = to_categorical(y)\n",
    "\n",
    "\n",
    "\n",
    "X_train_l, X_test_l,y_train_l,y_test_l = train_test_split(X,y, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "929985a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_l = StandardScaler()\n",
    "\n",
    "X_train_sc_l = sc_l.fit_transform(X_train_l)\n",
    "X_test_sc_l = sc_l.transform(X_test_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4011f42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.1694 - acc: 0.1918 - val_loss: 2.0957 - val_acc: 0.2264\n",
      "Epoch 2/30\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.0756 - acc: 0.2335 - val_loss: 2.0352 - val_acc: 0.2547\n",
      "Epoch 3/30\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 2.0115 - acc: 0.2628 - val_loss: 1.9861 - val_acc: 0.2784\n",
      "Epoch 4/30\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 1.9505 - acc: 0.2911 - val_loss: 1.9394 - val_acc: 0.2979\n",
      "Epoch 5/30\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.9029 - acc: 0.3126 - val_loss: 1.9087 - val_acc: 0.3104\n",
      "Epoch 6/30\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.8620 - acc: 0.3283 - val_loss: 1.8854 - val_acc: 0.3200\n",
      "Epoch 7/30\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 1.8268 - acc: 0.3425 - val_loss: 1.8648 - val_acc: 0.3288\n",
      "Epoch 8/30\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.7988 - acc: 0.3524 - val_loss: 1.8546 - val_acc: 0.3332\n",
      "Epoch 9/30\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.7742 - acc: 0.3616 - val_loss: 1.8438 - val_acc: 0.3376\n",
      "Epoch 10/30\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.7530 - acc: 0.3695 - val_loss: 1.8272 - val_acc: 0.3440\n",
      "Epoch 11/30\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.7302 - acc: 0.3781 - val_loss: 1.8267 - val_acc: 0.3462\n",
      "Epoch 12/30\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.7150 - acc: 0.3845 - val_loss: 1.8181 - val_acc: 0.3492\n",
      "Epoch 13/30\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.6961 - acc: 0.3926 - val_loss: 1.8105 - val_acc: 0.3536\n",
      "Epoch 14/30\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 1.6800 - acc: 0.3977 - val_loss: 1.8054 - val_acc: 0.3565\n",
      "Epoch 15/30\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.6658 - acc: 0.4032 - val_loss: 1.8050 - val_acc: 0.3571\n",
      "Epoch 16/30\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.6542 - acc: 0.4074 - val_loss: 1.8002 - val_acc: 0.3613\n",
      "Epoch 17/30\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 1.6396 - acc: 0.4118 - val_loss: 1.8046 - val_acc: 0.3608\n",
      "Epoch 18/30\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.6301 - acc: 0.4175 - val_loss: 1.7987 - val_acc: 0.3624\n",
      "Epoch 19/30\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.6139 - acc: 0.4220 - val_loss: 1.8040 - val_acc: 0.3623\n",
      "Epoch 20/30\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.6029 - acc: 0.4267 - val_loss: 1.7966 - val_acc: 0.3663\n",
      "Epoch 21/30\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 1.5969 - acc: 0.4275 - val_loss: 1.8022 - val_acc: 0.3662\n",
      "Epoch 22/30\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 1.5869 - acc: 0.4323 - val_loss: 1.8028 - val_acc: 0.3681\n",
      "Epoch 23/30\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.5786 - acc: 0.4344 - val_loss: 1.7926 - val_acc: 0.3710\n",
      "Epoch 24/30\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.5670 - acc: 0.4397 - val_loss: 1.8049 - val_acc: 0.3685\n",
      "Epoch 25/30\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.5592 - acc: 0.4431 - val_loss: 1.8049 - val_acc: 0.3696\n",
      "Epoch 26/30\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.5502 - acc: 0.4453 - val_loss: 1.8006 - val_acc: 0.3705\n",
      "Epoch 27/30\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 1.5422 - acc: 0.4478 - val_loss: 1.8062 - val_acc: 0.3698\n",
      "Epoch 28/30\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.5354 - acc: 0.4518 - val_loss: 1.8076 - val_acc: 0.3717\n",
      "Epoch 29/30\n",
      "2495/2495 [==============================] - 13s 5ms/step - loss: 1.5274 - acc: 0.4542 - val_loss: 1.8122 - val_acc: 0.3699\n",
      "Epoch 30/30\n",
      "2495/2495 [==============================] - 12s 5ms/step - loss: 1.5223 - acc: 0.4556 - val_loss: 1.8145 - val_acc: 0.3726\n"
     ]
    }
   ],
   "source": [
    "# let's create our model\n",
    "nn3= Sequential()\n",
    "# 1st hidden layer\n",
    "nn3.add(Dense(512, input_dim=X_train_sc_l.shape[1], activation='relu'))\n",
    "nn3.add(Dense(512, activation='relu'))\n",
    "nn3.add(Dense(512, activation='relu'))\n",
    "\n",
    "# output layer\n",
    "nn3.add(Dense(11, activation='softmax'))\n",
    "\n",
    "nn3.compile(loss ='categorical_crossentropy', optimizer= 'adam', metrics=['acc'])\n",
    "\n",
    "results3 = nn3.fit(X_train_sc_l, y_train_l,\n",
    "                    batch_size=256,\n",
    "                    epochs=30,\n",
    "                    validation_data=(X_test_sc_l,y_test_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30be7a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
